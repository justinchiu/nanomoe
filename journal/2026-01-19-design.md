Started initial design brainstorm. Simplify everything around one batch contract.

---

## Simplified API (current)

- Single batch contract: `PackedBatch`.
- All data sources (pretrain/SFT/RL) yield `PackedBatch`.
- Training loop consumes `PackedBatch` only; loss uses `token_weights`.

```python
@dataclass
class PackedBatch:
    tokens: Tensor
    position_ids: Tensor
    cu_seqlens: Tensor
    token_weights: Tensor        # per-token weight (mask/advantage)
    labels: Tensor | None = None
    log_probs: Tensor | None = None
    rewards: Tensor | None = None

class BatchSource(Protocol):
    def __iter__(self) -> Iterator[PackedBatch]: ...
```

---

## Task mappings

### Pretrain

- `token_weights = 1`
- `labels = shift(tokens)`
- Dataset yields packed batches directly.

### SFT

- `token_weights = loss_mask`
- `labels = shift(tokens)`
- Dataset yields packed batches directly.

### RL

```python
class Sampler(Protocol):
    def sample_and_score(self, prompt: Tensor) -> ScoredGroup: ...

@dataclass
class ScoredGroup:
    tokens: Tensor    # [N, seq_len]
    log_probs: Tensor # [N, gen_len]
    rewards: Tensor   # [N]
    prompt_len: int

class RLDataset:
    def __iter__(self) -> Iterator[PackedBatch]: ...
```

- `RLDataset` pulls groups, filters `reward_std <= eps`, computes advantages.
- `token_weights = advantages * mask`
- Guard with `max_attempts` to avoid infinite loops.
- Metrics: `zero_var_groups`, `valid_groups`, `attempts_per_batch`.

---

## Single-node focus

- Start dense (`num_experts=1` or DenseFFN), then scale experts on one H100.
- Keep APIs stable for later DDP on a single node.
- Favor bf16 + short iterations for fast experiments.
- Use a much smaller Qwen3-30B-A3B-style MoE architecture.

---

## Sampler, logging, checkpointing

- Sampler is stateless; `RLDataset` pulls, filters, computes advantages, packs.
- Prefer sync pull loop; add queue only if sampling is the bottleneck.
- Logging in training loop: `train/*`, `rollout/*`, `perf/*`.
- Checkpoint `data_state` (streaming epoch/samples_seen; RL skip stats + RNG if needed).

---

# Existing Repo Analysis

Surveyed 4 repos across data, trainer, sampler axes and their interfaces.

## 1. slime

**Path:** `/data/junxiong/slime/slime`

**Data:**
- `Sample` dataclass (`slime/utils/types.py:8-99`) - rich per-sample struct with `prompt`, `tokens`, `response`, `reward`, `loss_mask`, `rollout_log_probs`, `rollout_routed_experts`
- `RolloutBatch = dict[str, list[Tensor]]` - dictionary batch for training
- Data flow: `RolloutDataSource.get_samples()` → async generation → `_convert_samples_to_train_data()` → `_split_train_data_by_dp()` → Ray Box refs

**Trainer:**
- Ray-distributed actors: `MegatronTrainRayActor` (`slime/backends/megatron_utils/actor.py:45+`)
- Main loop in `train.py:16-106`: rollout → train → update weights → checkpoint
- `train_actor()` (lines 385-540): compute log_probs → advantages → fwd/bwd loop
- Megatron backend for TP/DP/CP/VPP parallelism

**Sampler:**
- `GenerateState` singleton (`slime/rollout/sglang_rollout.py:112-162`) with SGLang backend
- `generate_rollout_async()` (lines 452-533): asyncio gather over sample groups
- Returns `RolloutFnTrainOutput(samples: list[list[Sample]], metrics: dict)`

**Interface:**
- Rollout → Training: `list[Box[RolloutBatch]]` via Ray object store
- Weight sync: `weights_backuper.backup()` → `weight_updater.update_weights()` to rollout engines

---

## 2. Megatron-LM

**Path:** `/data/junxiong/Megatron-LM`

**Data:**
- `GPTDataset.__getitem__()` (`megatron/core/datasets/gpt_dataset.py:228-299`) returns dict: `tokens`, `labels`, `attention_mask`, `loss_mask`, `position_ids`
- `MegatronPretrainingSampler` (`megatron/training/datasets/data_samplers.py:96-163`) - distributes indices across DP ranks
- Batching via `torch.utils.data.DataLoader`

**Trainer:**
- `train_step()` (`megatron/training/training.py:1592-1720`): zero_grad → forward_backward_func → optimizer.step → lr_scheduler.step
- `get_forward_backward_func()` (`schedules.py:45`) selects pipeline schedule (interleaved, pipelining, no-pipelining)
- Main loop `train()` (line 2335): while iteration < train_iters

**Sampler:**
- `SamplingParams` dataclass (`megatron/core/inference/sampling_params.py:8-88`)
- `DynamicInferenceRequest` (`inference_request.py:236-276`) with status lifecycle
- `Scheduler` (`scheduler.py:17-150`) manages active/waiting/completed pools
- `TextGenerationController` (`text_generation_controller.py:46-120`) orchestrates sampling loop

**Interface:**
- Training: `forward_step_func(data_iterator, model)` → consumes batch dict
- Inference: `Scheduler.add_request()` → `DynamicInferenceEngine` step loop → `InferenceBatchDimensions` for CUDA graph matching

---

## 3. tinker-cookbook

**Path:** `/data/junxiong/tinker-cookbook`

**Data:**
- `tinker.Datum` - contains `model_input: ModelInput` + `loss_fn_inputs: dict`
- SFT `loss_fn_inputs`: `{"weights": TensorData, "target_tokens": TensorData}` (`supervised/common.py:66-138`)
- RL `loss_fn_inputs`: `{"target_tokens", "logprobs", "advantages", "mask"}` (`rl/data_processing.py:134-142`)
- `SupervisedDataset` protocol (`supervised/types.py:15-36`): `get_batch(idx) → list[Datum]`
- `RLDataset` protocol (`rl/types.py:152-164`): `get_batch(idx) → Sequence[EnvGroupBuilder]`
- `Renderer.build_supervised_example()` (`renderers/base.py:774-900`) creates ModelInput + weights from messages

**Trainer:**
- SFT loop (`supervised/train.py:148-350`): `dataset.get_batch()` → `forward_backward()` → `optim_step()`
- RL loop (`rl/train.py:181-227`): pipelined substeps with async futures
- Pipelining: overlap fwd_bwd and optim_step via futures

**Sampler:**
- `TokenCompleter` protocol (`completers.py:33-38`): `async __call__(model_input, stop) → TokensWithLogprobs`
- `TinkerTokenCompleter` (lines 49-79) wraps `sampling_client.sample_async()`
- `do_single_rollout()` (`rl/rollouts.py:27-46`): observation → policy → env.step loop
- `do_group_rollout()` (lines 50-115): parallel rollouts + group reward computation

**Interface:**
- Rollout → Training: `TrajectoryGroup` → `assemble_training_data()` → `list[Datum]`
- Group rewards via `EnvGroupBuilder.compute_group_rewards()` enables GRPO-style variance reduction

---

## 4. nmoe

**Path:** `/data/junxiong/nmoe`

**Data:**
- Simple tuple: `(inputs, targets)` both `torch.Tensor[batch_size, seq_len]` (`data/loader.py:231-254`)
- `Cursor` dataclass (`data/dataset.py:19-86`) tracks position in memory-mapped .npy shards
- `DeterministicLoader` (`data/loader.py:32-260`) with SWRR source selection and exact-resume state
- `MixturePlan` (`data/mixture.py:23-99`) for multi-stage source weighting

**Trainer:**
- Single file loop (`train.py:36-185`): `loader.next()` → `model(inputs)` → masked cross-entropy → backward → step
- Loss masking: ignores EOS tokens (`train.py:102-105`)
- Checkpoint includes loader state dict for exact resume

**Sampler:**
- `_generate_lockstep()` (`eval/run_gen.py:141-195`): distributed-safe generation with padding
- Chunked vocab operations for memory efficiency (`_topk_vocab_chunked`, `_argmax_vocab_chunked`)
- Choice-based eval (`eval/choices.py:51-311`) with `_logsumexp_vocab_chunked()` scoring

**Interface:**
- Loader → Trainer: direct `(inputs, targets)` tuple consumption
- Trainer → Sampler: frozen checkpoint → distributed lockstep inference

---

## Comparison Table

| Aspect | slime | Megatron-LM | tinker | nmoe |
|--------|-------|-------------|--------|------|
| **Batch type** | `dict[str, list[Tensor]]` | `dict` with keys | `list[Datum]` | `(Tensor, Tensor)` tuple |
| **Per-token weights** | `loss_mask` in Sample | `loss_mask` in batch dict | `weights`/`mask` in loss_fn_inputs | EOS masking in loss |
| **RL advantages** | Computed post-rollout | N/A (pretrain focus) | `advantages` in loss_fn_inputs | N/A |
| **Sampler** | Async SGLang + Ray | Scheduler + CUDA graphs | `TokenCompleter` protocol | Lockstep distributed |
| **Distributed** | Ray actors + Megatron | Tensor/Pipeline/Data parallel | Async futures | Simple DP |
| **Log probs** | `rollout_log_probs` on Sample | InferenceRequest | `logprobs` in loss_fn_inputs | N/A |

---

## Design Insights for nanomoe

1. **Batch contract**: tinker's `Datum` with `loss_fn_inputs` dict is closest to our `PackedBatch` with `token_weights`. slime's `Sample` is per-sequence, converted to batch later.

2. **Weight semantics**: tinker separates `mask` (binary) and `advantages` (float) in RL but uses single `weights` (binary) for SFT. We pre-multiply: `token_weights = advantages * mask`.

3. **Sampler statefulness**: slime/tinker use stateless async samplers that return scored sequences. nmoe uses simple lockstep generation. Our `Sampler.sample_and_score()` → `ScoredGroup` aligns with slime/tinker.

4. **Filtering zero-variance groups**: tinker handles this in `EnvGroupBuilder.compute_group_rewards()`. Our `RLDataset` pull-loop with `max_attempts` guard is similar.

5. **Packing**: Only Megatron and nanomoe explicitly handle sequence packing with `cu_seqlens`. Others assume fixed-length or per-sample processing.
