Started initial design brainstorm. Simplify everything around one batch contract.

---

## Simplified API (current)

- Single batch contract: `PackedBatch`.
- All data sources (pretrain/SFT/RL) yield `PackedBatch`.
- Training loop consumes `PackedBatch` only; loss uses `token_weights`.

```python
@dataclass
class PackedBatch:
    tokens: Tensor
    position_ids: Tensor
    cu_seqlens: Tensor
    token_weights: Tensor        # per-token weight (mask/advantage)
    labels: Tensor | None = None
    log_probs: Tensor | None = None
    rewards: Tensor | None = None

class BatchSource(Protocol):
    def __iter__(self) -> Iterator[PackedBatch]: ...
```

---

## Task mappings

### Pretrain

- `token_weights = 1`
- `labels = shift(tokens)`
- Dataset yields packed batches directly.

### SFT

- `token_weights = loss_mask`
- `labels = shift(tokens)`
- Dataset yields packed batches directly.

### RL

```python
class Sampler(Protocol):
    def sample_and_score(self, prompt: Tensor) -> ScoredGroup: ...

@dataclass
class ScoredGroup:
    tokens: Tensor    # [N, seq_len]
    log_probs: Tensor # [N, gen_len]
    rewards: Tensor   # [N]
    prompt_len: int

class RLDataset:
    def __iter__(self) -> Iterator[PackedBatch]: ...
```

- `RLDataset` pulls groups, filters `reward_std <= eps`, computes advantages.
- `token_weights = advantages * mask`
- Guard with `max_attempts` to avoid infinite loops.
- Metrics: `zero_var_groups`, `valid_groups`, `attempts_per_batch`.

---

## Single-node focus

- Start dense (`num_experts=1` or DenseFFN), then scale experts on one H100.
- Keep APIs stable for later DDP on a single node.
- Favor bf16 + short iterations for fast experiments.
- Use a much smaller Qwen3-30B-A3B-style MoE architecture.

---

## Sampler, logging, checkpointing

- Sampler is stateless; `RLDataset` pulls, filters, computes advantages, packs.
- Prefer sync pull loop; add queue only if sampling is the bottleneck.
- Logging in training loop: `train/*`, `rollout/*`, `perf/*`.
- Checkpoint `data_state` (streaming epoch/samples_seen; RL skip stats + RNG if needed).

---

# Existing Repo Analysis

Surveyed 4 repos across data, trainer, sampler axes and their interfaces.

## 1. slime

**Path:** `/data/junxiong/slime/slime`

**Data:**
- `Sample` includes prompt/response, `loss_mask`, rollout log-probs, routed expert info, and metadata.
- `RolloutBatch` is a dict of per-sample lists (tensors/ints/floats/strings) used by the trainer.
- Data flow: `RolloutDataSource.get_samples()` → async rollout → `_convert_samples_to_train_data()` → `_split_train_data_by_dp()` → `Box(ray.put(...))`.
- Uses packed sequence params (`PackedSeqParams` with `cu_seqlens`) and seqlen balancing inside the training data iterator.

**Trainer:**
- Ray actors (`MegatronTrainRayActor`/`FSDPTrainRayActor`) own the step loop.
- Megatron backend uses CP + packed sequences; advantages/returns computed in `loss.py`.
- Rollout → train → weight sync → checkpoint pattern per iteration.

**Sampler:**
- SGLang rollout with `GenerateState` (singleton) and asyncio tasks.
- `generate_rollout_async()` oversamples, applies dynamic filters, and returns `RolloutFnTrainOutput` plus any aborted groups.

**Interface:**
- Rollout → Training: `list[Box[RolloutBatch]]` via Ray object store
- Weight sync: `weights_backuper.backup()` → `weight_updater.update_weights()` to rollout engines

---

## 2. Megatron-LM

**Path:** `/data/junxiong/Megatron-LM`

**Data:**
- `GPTDataset.__getitem__()` returns dict: `tokens`, `labels`, `attention_mask`, `loss_mask`, `position_ids`.
- `MegatronPretrainingSampler` (`megatron/training/datasets/data_samplers.py:96-163`) - distributes indices across DP ranks
- Batching via `torch.utils.data.DataLoader`

**Trainer:**
- `train_step()` calls `forward_backward_func()` with micro-batches, then optimizer + LR scheduler.
- Pipeline scheduling is selected by `get_forward_backward_func()` (interleaved/pipelining variants).
- Main loop is iterative and centralized.

**Sampler:**
- `SamplingParams` defines request-level controls.
- `Scheduler` manages active/waiting/completed requests; `DynamicInferenceRequest` tracks lifecycle.
- `TextGenerationController` orchestrates prefill/decode and CUDA-graph batch dimensions.

**Interface:**
- Training: `forward_step_func(data_iterator, model)` → consumes batch dict
- Inference: `Scheduler.add_request()` → inference engine step loop with `InferenceBatchDimensions`.

---

## 3. tinker-cookbook

**Path:** `/data/junxiong/tinker-cookbook`

**Data:**
- `tinker.Datum` = `model_input` + `loss_fn_inputs`.
- SFT uses `weights` + `target_tokens`; RL uses `target_tokens`, `logprobs`, `advantages`, `mask`.
- `SupervisedDataset.get_batch()` returns `list[Datum]`; `RLDataset.get_batch()` returns `Sequence[EnvGroupBuilder]`.
- Streaming supervised datasets exist for HF inputs.

**Trainer:**
- SFT loop: `get_batch()` → `forward_backward()` → `optim_step()`.
- RL loop overlaps rollout/advantage processing with training via async futures.

**Sampler:**
- `TokenCompleter` is the RL-facing async token interface; `TinkerTokenCompleter` wraps a SamplingClient.
- `do_group_rollout()` runs group envs and computes group rewards.

**Interface:**
- Rollout → Training: `TrajectoryGroup` → `assemble_training_data()` → `list[Datum]`
- Group rewards come from `EnvGroupBuilder.compute_group_rewards()`.

---

## 4. nmoe

**Path:** `/data/junxiong/nmoe`

**Data:**
- Simple tuple: `(inputs, targets)` both `torch.Tensor[batch_size, seq_len]` (`data/loader.py:231-254`)
- `Cursor` dataclass (`data/dataset.py:19-86`) tracks position in memory-mapped .npy shards
- `DeterministicLoader` (`data/loader.py:32-260`) with SWRR source selection and exact-resume state
- `MixturePlan` (`data/mixture.py:23-99`) for multi-stage source weighting
- Prefetch queue is optional; resume uses loader state dict.

**Trainer:**
- Single file loop: `loader.next()` → forward → masked cross-entropy → backward → step.
- Loss masking: ignores EOS tokens (`train.py:102-105`)
- Checkpoint includes loader state dict for exact resume

**Sampler:**
- `_generate_lockstep()` (`eval/run_gen.py:141-195`): distributed-safe generation with padding
- Chunked vocab operations for memory efficiency (`_topk_vocab_chunked`, `_argmax_vocab_chunked`)
- Choice-based eval (`eval/choices.py:51-311`) with `_logsumexp_vocab_chunked()` scoring

**Interface:**
- Loader → Trainer: direct `(inputs, targets)` tuple consumption
- Trainer → Sampler: frozen checkpoint → distributed lockstep inference

---

## Comparison Table

| Aspect | slime | Megatron-LM | tinker | nmoe |
|--------|-------|-------------|--------|------|
| **Batch type** | `dict` of per-sample lists | `dict` with keys | `list[Datum]` | `(Tensor, Tensor)` tuple |
| **Per-token weights** | `loss_mask` in Sample | `loss_mask` in batch dict | `weights`/`mask` in loss_fn_inputs | EOS masking in loss |
| **RL advantages** | Computed post-rollout | N/A (pretrain focus) | `advantages` in loss_fn_inputs | N/A |
| **Sampler** | Async SGLang + Ray | Scheduler + CUDA graphs | `TokenCompleter` protocol | Lockstep distributed |
| **Distributed** | Ray actors + Megatron | Tensor/Pipeline/Data parallel | Async futures | Simple DP |
| **Log probs** | `rollout_log_probs` on Sample | InferenceRequest | `logprobs` in loss_fn_inputs | N/A |

---

## Design Insights for nanomoe

1. **Batch contract**: tinker's `Datum` with `loss_fn_inputs` dict is closest to our `PackedBatch` with `token_weights`. slime's `Sample` is per-sequence, converted to batch later.

2. **Weight semantics**: tinker separates `mask` (binary) and `advantages` (float) in RL but uses single `weights` (binary) for SFT. We pre-multiply: `token_weights = advantages * mask`.

3. **Sampler statefulness**: slime/tinker expose stateless sampler interfaces, with async state inside the rollout engine. nmoe uses lockstep generation.

4. **Filtering zero-variance groups**: tinker filters constant-reward groups during RL data processing; our `RLDataset` pull-loop with `max_attempts` is the same idea.

5. **Packing**: slime uses packed sequence params + `cu_seqlens`; Megatron relies on masks; nmoe uses fixed-length windows.

---

# Deeper Step-by-Step View

## slime

- Prompt groups built in `RolloutDataSource.get_samples()` (clone per prompt, tag group/index).
- `generate_rollout_async()` submits sampling tasks, applies dynamic filters, collects valid groups.
- Output wrapped as `RolloutFnTrainOutput(samples=..., metrics=...)` (plus aborted groups).
- Samples → training dict via `_convert_samples_to_train_data()` then `_split_train_data_by_dp()`.
- `RolloutBatch` stored in Ray object store (`Box(ray.put(...))`).
- Trainer uses `DataIterator` + `PackedSeqParams` (`cu_seqlens`) and computes advantages in `loss.py`.
- Loop: rollout → train → weight sync → checkpoint.

## Megatron-LM

- `GPTDataset.__getitem__()` returns dict: `tokens`, `labels`, `loss_mask`, `position_ids` (+ attention mask).
- `MegatronPretrainingSampler` shards indices; `DataLoader` yields batch dicts.
- `train_step()` runs `forward_backward_func()` on micro-batches, then optimizer + LR schedule.
- `get_forward_backward_func()` selects pipeline schedule; outer `train()` loops steps.
- Inference: `Scheduler.add_request()` manages request pools; `DynamicInferenceRequest` tracks lifecycle.
- `TextGenerationController` drives prefill/decode with `InferenceBatchDimensions`.

## tinker-cookbook

- SFT: `SupervisedDataset.get_batch()` returns `list[Datum]` with `model_input` + `loss_fn_inputs`.
- RL: `RLDataset.get_batch()` returns `Sequence[EnvGroupBuilder]`.
- Rollout: `TokenCompleter` async generates tokens+logprobs; `do_group_rollout()` runs group envs.
- Trajectories → `Datum` via `trajectory_to_data()`/`assemble_training_data()`.
- SFT loop: `get_batch()` → `forward_backward()` → `optim_step()`.
- RL loop overlaps rollout + training with async futures.

## nmoe

- `DeterministicLoader` yields `(inputs, targets)` with exact-resume cursor state.
- Optional prefetch queue; deterministic source selection per step.
- Training loop: `loader.next()` → forward → masked CE → backward → step → log → checkpoint.
- Checkpoint includes loader state for exact resume.
- Sampling uses `_generate_lockstep()` with chunked vocab ops.

---

## Concrete Patterns to Keep

- Single batch contract; datasets yield `PackedBatch`, trainer stays dumb.
- Stateless sampler; `RLDataset` handles filtering, advantages, packing.
- Exact-resume data state (cursor/epoch/offset) for streaming + RL prompts.
- Packing as a dataset concern; trainer only consumes packed batches.
- Start sync; add async queue only if sampling is the bottleneck.
- Centralized logging in the training loop with `train/*` and `rollout/*`.

---

## Why Unified Loss Works for nanomoe

**tinker can't fully unify** because it uses different loss functions:
- SFT: `loss_fn="cross_entropy"` — standard next-token prediction
- RL: `loss_fn="importance_sampling"` — needs `π(a)/π_old(a)` ratio, requires old log_probs

That's why tinker keeps `weights` (SFT) separate from `mask + advantages + logprobs` (RL).

**nanomoe CAN unify** because GRPO is NOT importance sampling. It's on-policy: sample from current policy, train immediately. No π_old correction needed.

Our GRPO loss (from `train/grpo.py`):
```python
policy_loss = -(advantages * log_probs * loss_mask).sum() / loss_mask.sum()
```

This is just **weighted cross-entropy**. We can write a single loss:
```python
def unified_loss(logits, labels, token_weights):
    log_probs = -F.cross_entropy(logits, labels, reduction='none')
    return -(token_weights * log_probs).sum() / token_weights.abs().sum()
```

**Task mappings**:
| Task | `token_weights` | Result |
|------|-----------------|--------|
| Pretrain | `1.0` | Standard LM loss |
| SFT | `loss_mask` (0/1) | Mask prompt tokens |
| RL (GRPO) | `advantages * mask` | Policy gradient with group-relative rewards |

**What this enables**:
- ONE `train_step()` for all tasks
- No task-type branching in training loop
- Dataset is responsible for setting `token_weights` appropriately

**RL-specific fields** (`log_probs`, `rewards`) are only for metrics/logging, not loss computation.

---

# Implementation Plan

## Audit Findings

**Problem: Two conflicting `PackedBatch` definitions**

`data/types.py` (GRPO):
```python
tokens, loss_mask, position_ids, cu_seqlens, log_probs, advantages, rewards
```

`data/packed_dataset.py` (Pretrain):
```python
input_ids, labels, position_ids, cu_seqlens, loss_mask
```

**Field mismatches:**
| Pretrain | GRPO | Unified |
|----------|------|---------|
| `input_ids` | `tokens` | `tokens` |
| `labels` | (inline) | `labels` |
| `loss_mask` | `loss_mask` + `advantages` | `token_weights` |

**Good news:** `grpo_loss()` already implements unified pattern:
```python
loss = -(expanded_advantages * masked_log_probs).sum() / (response_mask.sum() + 1e-8)
```

---

## Target State

Single `PackedBatch` in `data/types.py`:
```python
@dataclass
class PackedBatch:
    tokens: Tensor           # [total_tokens]
    labels: Tensor           # [total_tokens]
    position_ids: Tensor     # [total_tokens]
    cu_seqlens: Tensor       # [num_seqs + 1]
    token_weights: Tensor    # [total_tokens] - 1.0/mask/advantages*mask
    # Optional (metrics/logging only)
    log_probs: Tensor | None = None
    rewards: Tensor | None = None
```

Single `unified_loss()` in `train/loss.py`:
```python
def unified_loss(logits, batch):
    log_probs = -F.cross_entropy(logits.view(-1, V), batch.labels.view(-1), reduction='none')
    return -(batch.token_weights * log_probs).sum() / batch.token_weights.abs().sum().clamp(min=1)
```

---

## Implementation Steps

### Step 1: Unify `PackedBatch` (`data/types.py`)
- Merge both definitions into one
- Rename `input_ids` → `tokens`
- Add `labels` field
- Replace `loss_mask` + `advantages` → `token_weights`
- Keep `log_probs`, `rewards` as optional

### Step 2: Update `packed_dataset.py`
- Delete local `PackedBatch` class
- Import from `types.py`
- Set `token_weights = loss_mask.float()` (pretrain: all 1s except first token)
- Add `labels` field (already computed)

### Step 3: Update `packing.py`
- Compute `token_weights = advantages * mask` during packing
- Remove separate `advantages` field
- Add `labels = tokens[1:] + [pad]` during packing

### Step 4: Add `unified_loss()` (`train/loss.py`)
- New file with single loss function
- Handles all tasks via `token_weights`

### Step 5: Simplify `grpo.py`
- Remove inline loss computation
- Use `unified_loss()`
- Keep advantage computation in `compute_grpo_advantages()`

### Step 6: Update `scripts/pretrain.py`
- Use unified `PackedBatch`
- Use `unified_loss()`

### Step 7: Cleanup
- Delete `PretrainBatch` alias from `data/__init__.py`
- Update imports across codebase

---

## Files Changed

| File | Change |
|------|--------|
| `data/types.py` | Merge `PackedBatch`, add `token_weights`, `labels` |
| `data/packed_dataset.py` | Delete local class, use unified, set `token_weights` |
| `data/packing.py` | Compute `token_weights`, add `labels` |
| `data/__init__.py` | Remove `PretrainBatch` alias |
| `train/loss.py` | New file with `unified_loss()` |
| `train/grpo.py` | Use `unified_loss()`, simplify |
| `train/__init__.py` | Export `unified_loss` |
| `scripts/pretrain.py` | Use unified batch + loss |

Estimated: ~200 lines changed across 8 files.
